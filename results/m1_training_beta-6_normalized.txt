n-62-12-19(s151988) $ python code/m1_trainer.py 
# Loading GTEx data...
# Loading GTEx data...
use_cuda=True
M1_model(
  (regressor): Regressor(
    (h_layers): Sequential(
      (0): Linear(in_features=256, out_features=2048, bias=True)
      (1): ReLU()
      (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=2048, out_features=2048, bias=True)
      (5): ReLU()
      (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): Dropout(p=0.2, inplace=False)
    )
    (out_layer): Linear(in_features=2048, out_features=156958, bias=True)
    (out_activation): ReLU()
  )
  (VAE): VariationalAutoencoder(
    (encoder): Encoder(
      (hidden): ModuleList(
        (0): Linear(in_features=18965, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=512, bias=True)
      )
      (sample): GaussianSample(
        (mu): Linear(in_features=512, out_features=256, bias=True)
        (log_var): Linear(in_features=512, out_features=256, bias=True)
      )
    )
    (decoder): Decoder(
      (hidden): ModuleList(
        (0): Linear(in_features=256, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=512, bias=True)
      )
      (reconstruction): Linear(in_features=512, out_features=18965, bias=True)
      (output_activation): ReLU()
    )
  )
)
Epoch 1/200	Training loss:	0.9817	Validation loss:	0.8040

Epoch 2/200	Training loss:	0.8571	Validation loss:	0.6054

Epoch 3/200	Training loss:	1.2510	Validation loss:	0.4879

Epoch 4/200	Training loss:	0.4523	Validation loss:	0.4709

Epoch 5/200	Training loss:	0.6830	Validation loss:	0.4115

Epoch 6/200	Training loss:	0.4823	Validation loss:	0.4000

Epoch 7/200	Training loss:	0.8890	Validation loss:	0.3964

Epoch 8/200	Training loss:	1.1207	Validation loss:	0.3557

Epoch 9/200	Training loss:	0.8025	Validation loss:	0.3413

Epoch 10/200	Training loss:	0.5523	Validation loss:	0.3087

Epoch 11/200	Training loss:	0.8857	Validation loss:	0.3262

Epoch 12/200	Training loss:	0.7772	Validation loss:	0.2778

Epoch 13/200	Training loss:	0.9071	Validation loss:	0.2679

Epoch 14/200	Training loss:	0.6403	Validation loss:	0.2871

Epoch 15/200	Training loss:	0.5392	Validation loss:	0.2496

Epoch 16/200	Training loss:	0.5253	Validation loss:	0.2592

Epoch 17/200	Training loss:	0.8268	Validation loss:	0.2118

Epoch 18/200	Training loss:	0.5985	Validation loss:	0.1750

Epoch 19/200	Training loss:	0.5572	Validation loss:	0.1697

Epoch 20/200	Training loss:	0.6919	Validation loss:	0.1763

Epoch 21/200	Training loss:	0.6666	Validation loss:	0.1893

Epoch 22/200	Training loss:	0.4959	Validation loss:	0.1741

Epoch 23/200	Training loss:	0.3638	Validation loss:	0.1724

Epoch 24/200	Training loss:	0.3442	Validation loss:	0.1492

Epoch 25/200	Training loss:	0.4982	Validation loss:	0.1660

Epoch 26/200	Training loss:	0.3413	Validation loss:	0.1511

Epoch 27/200	Training loss:	0.4655	Validation loss:	0.1361

Epoch 28/200	Training loss:	0.3802	Validation loss:	0.1702

Epoch 29/200	Training loss:	0.3015	Validation loss:	0.1610

Epoch 30/200	Training loss:	0.2460	Validation loss:	0.1611

Epoch 31/200	Training loss:	0.5704	Validation loss:	0.1487

Epoch 32/200	Training loss:	0.4775	Validation loss:	0.1503

Epoch 33/200	Training loss:	0.2661	Validation loss:	0.1339

Epoch 34/200	Training loss:	0.2139	Validation loss:	0.1389

Epoch 35/200	Training loss:	0.2141	Validation loss:	0.1488

Epoch 36/200	Training loss:	0.2638	Validation loss:	0.1341

Epoch 37/200	Training loss:	0.2831	Validation loss:	0.1599

Epoch 38/200	Training loss:	0.2400	Validation loss:	0.1346

Epoch 39/200	Training loss:	0.1690	Validation loss:	0.1292

Epoch 40/200	Training loss:	0.2972	Validation loss:	0.1311

Epoch 41/200	Training loss:	0.2006	Validation loss:	0.1269

Epoch 42/200	Training loss:	0.1881	Validation loss:	0.1262

Epoch 43/200	Training loss:	0.2168	Validation loss:	0.1269

Epoch 44/200	Training loss:	0.3124	Validation loss:	0.1274

Epoch 45/200	Training loss:	0.1967	Validation loss:	0.1267

Epoch 46/200	Training loss:	0.2426	Validation loss:	0.1266

Epoch 47/200	Training loss:	0.1883	Validation loss:	0.1255

Epoch 48/200	Training loss:	0.1511	Validation loss:	0.1262

Epoch 49/200	Training loss:	0.1718	Validation loss:	0.1264

Epoch 50/200	Training loss:	0.1752	Validation loss:	0.1273

Epoch 51/200	Training loss:	0.1952	Validation loss:	0.1272

Epoch 52/200	Training loss:	0.1619	Validation loss:	0.1273

Epoch 53/200	Training loss:	0.2004	Validation loss:	0.1269

Epoch 54/200	Training loss:	0.1689	Validation loss:	0.1267

Epoch 55/200	Training loss:	0.1946	Validation loss:	0.1257

Epoch 56/200	Training loss:	0.1830	Validation loss:	0.1252

Epoch 57/200	Training loss:	0.1886	Validation loss:	0.1255

Epoch 58/200	Training loss:	0.2003	Validation loss:	0.1244

Epoch 59/200	Training loss:	0.1845	Validation loss:	0.1259

Epoch 60/200	Training loss:	0.1704	Validation loss:	0.1245

Epoch 61/200	Training loss:	0.1659	Validation loss:	0.1242

Epoch 62/200	Training loss:	0.1928	Validation loss:	0.1247

Epoch 63/200	Training loss:	0.1316	Validation loss:	0.1235

Epoch 64/200	Training loss:	0.2668	Validation loss:	0.1244

Epoch 65/200	Training loss:	0.1920	Validation loss:	0.1240

Epoch 66/200	Training loss:	0.1869	Validation loss:	0.1238

Epoch 67/200	Training loss:	0.1691	Validation loss:	0.1235

Epoch 68/200	Training loss:	0.1425	Validation loss:	0.1218

Epoch 69/200	Training loss:	0.1810	Validation loss:	0.1243

Epoch 70/200	Training loss:	0.1796	Validation loss:	0.1231

Epoch 71/200	Training loss:	0.1856	Validation loss:	0.1222

Epoch 72/200	Training loss:	0.1811	Validation loss:	0.1215

Epoch 73/200	Training loss:	0.1721	Validation loss:	0.1223

Epoch 74/200	Training loss:	0.1475	Validation loss:	0.1230

Epoch 75/200	Training loss:	0.1979	Validation loss:	0.1229

Epoch 76/200	Training loss:	0.2852	Validation loss:	0.1238

Epoch 77/200	Training loss:	0.2425	Validation loss:	0.1225

Epoch 78/200	Training loss:	0.2051	Validation loss:	0.1231

Epoch 79/200	Training loss:	0.1559	Validation loss:	0.1219

Epoch 80/200	Training loss:	0.1839	Validation loss:	0.1226

Epoch 81/200	Training loss:	0.1930	Validation loss:	0.1230

Epoch 82/200	Training loss:	0.1580	Validation loss:	0.1211

Epoch 83/200	Training loss:	0.1581	Validation loss:	0.1220

Epoch 84/200	Training loss:	0.1788	Validation loss:	0.1205

Epoch 85/200	Training loss:	0.2004	Validation loss:	0.1227

Epoch 86/200	Training loss:	0.2439	Validation loss:	0.1206

Epoch 87/200	Training loss:	0.1812	Validation loss:	0.1224

Epoch 88/200	Training loss:	0.2093	Validation loss:	0.1219

Epoch 89/200	Training loss:	0.1941	Validation loss:	0.1199

Epoch 90/200	Training loss:	0.1669	Validation loss:	0.1205

Epoch 91/200	Training loss:	0.1594	Validation loss:	0.1205

Epoch 92/200	Training loss:	0.1544	Validation loss:	0.1211

Epoch 93/200	Training loss:	0.1521	Validation loss:	0.1221

Epoch 94/200	Training loss:	0.1747	Validation loss:	0.1207

Epoch 95/200	Training loss:	0.1771	Validation loss:	0.1210

Epoch 96/200	Training loss:	0.1831	Validation loss:	0.1225

Epoch 97/200	Training loss:	0.1666	Validation loss:	0.1210

Epoch 98/200	Training loss:	0.1957	Validation loss:	0.1207

Epoch 99/200	Training loss:	0.2152	Validation loss:	0.1206

Epoch 100/200	Training loss:	0.2054	Validation loss:	0.1201

Epoch 101/200	Training loss:	0.1591	Validation loss:	0.1212

Epoch 102/200	Training loss:	0.2304	Validation loss:	0.1202

Epoch 103/200	Training loss:	0.1494	Validation loss:	0.1219

Epoch 104/200	Training loss:	0.1854	Validation loss:	0.1188

Epoch 105/200	Training loss:	0.1759	Validation loss:	0.1200

Epoch 106/200	Training loss:	0.1721	Validation loss:	0.1194

Epoch 107/200	Training loss:	0.1806	Validation loss:	0.1210

Epoch 108/200	Training loss:	0.1546	Validation loss:	0.1200

Epoch 109/200	Training loss:	0.1615	Validation loss:	0.1195

Epoch 110/200	Training loss:	0.1836	Validation loss:	0.1210

Epoch 111/200	Training loss:	0.1607	Validation loss:	0.1203

Epoch 112/200	Training loss:	0.2325	Validation loss:	0.1196

Epoch 113/200	Training loss:	0.2328	Validation loss:	0.1200

Epoch 114/200	Training loss:	0.2021	Validation loss:	0.1208

Epoch 115/200	Training loss:	0.1914	Validation loss:	0.1185

Epoch 116/200	Training loss:	0.2012	Validation loss:	0.1196

Epoch 117/200	Training loss:	0.1834	Validation loss:	0.1183

Epoch 118/200	Training loss:	0.2036	Validation loss:	0.1194

Epoch 119/200	Training loss:	0.2120	Validation loss:	0.1190

Epoch 120/200	Training loss:	0.1586	Validation loss:	0.1192

Epoch 121/200	Training loss:	0.2244	Validation loss:	0.1185

Epoch 122/200	Training loss:	0.1615	Validation loss:	0.1185

Epoch 123/200	Training loss:	0.2060	Validation loss:	0.1186

Epoch 124/200	Training loss:	0.1781	Validation loss:	0.1199

Epoch 125/200	Training loss:	0.1768	Validation loss:	0.1189

Epoch 126/200	Training loss:	0.1864	Validation loss:	0.1192

Epoch 127/200	Training loss:	0.1355	Validation loss:	0.1190

Epoch 128/200	Training loss:	0.1474	Validation loss:	0.1196

Epoch 129/200	Training loss:	0.2223	Validation loss:	0.1194

Epoch 130/200	Training loss:	0.1706	Validation loss:	0.1182

Epoch 131/200	Training loss:	0.1540	Validation loss:	0.1178

Epoch 132/200	Training loss:	0.1995	Validation loss:	0.1198

Epoch 133/200	Training loss:	0.1662	Validation loss:	0.1181

Epoch 134/200	Training loss:	0.1752	Validation loss:	0.1197

Epoch 135/200	Training loss:	0.1467	Validation loss:	0.1178

Epoch 136/200	Training loss:	0.1690	Validation loss:	0.1190

Epoch 137/200	Training loss:	0.1910	Validation loss:	0.1188

Epoch 138/200	Training loss:	0.1921	Validation loss:	0.1190

Epoch 139/200	Training loss:	0.1811	Validation loss:	0.1172

Epoch 140/200	Training loss:	0.1754	Validation loss:	0.1184

Epoch 141/200	Training loss:	0.1566	Validation loss:	0.1186

Epoch 142/200	Training loss:	0.1383	Validation loss:	0.1188

Epoch 143/200	Training loss:	0.1696	Validation loss:	0.1174

Epoch 144/200	Training loss:	0.1671	Validation loss:	0.1195

Epoch 145/200	Training loss:	0.1660	Validation loss:	0.1181

Epoch 146/200	Training loss:	0.2063	Validation loss:	0.1184

Epoch 147/200	Training loss:	0.1683	Validation loss:	0.1183

Epoch 148/200	Training loss:	0.1312	Validation loss:	0.1179

Epoch 149/200	Training loss:	0.1765	Validation loss:	0.1183

Epoch 150/200	Training loss:	0.1492	Validation loss:	0.1176

Epoch 151/200	Training loss:	0.1658	Validation loss:	0.1175

Epoch 152/200	Training loss:	0.1342	Validation loss:	0.1179

Epoch 153/200	Training loss:	0.1588	Validation loss:	0.1178

Epoch 154/200	Training loss:	0.1443	Validation loss:	0.1179

Epoch 155/200	Training loss:	0.1615	Validation loss:	0.1173

Epoch 156/200	Training loss:	0.1350	Validation loss:	0.1177

Epoch 157/200	Training loss:	0.2296	Validation loss:	0.1179

Epoch 158/200	Training loss:	0.1557	Validation loss:	0.1177

Epoch 159/200	Training loss:	0.1431	Validation loss:	0.1174

Epoch 160/200	Training loss:	0.1689	Validation loss:	0.1187

Epoch 161/200	Training loss:	0.1759	Validation loss:	0.1175

Epoch 162/200	Training loss:	0.1841	Validation loss:	0.1175

Epoch 163/200	Training loss:	0.1742	Validation loss:	0.1176

Epoch 164/200	Training loss:	0.1603	Validation loss:	0.1175

Epoch 165/200	Training loss:	0.1485	Validation loss:	0.1163

Epoch 166/200	Training loss:	0.2294	Validation loss:	0.1175

Epoch 167/200	Training loss:	0.1904	Validation loss:	0.1176

Epoch 168/200	Training loss:	0.1686	Validation loss:	0.1190

Epoch 169/200	Training loss:	0.1582	Validation loss:	0.1180

Epoch 170/200	Training loss:	0.1852	Validation loss:	0.1180

Epoch 171/200	Training loss:	0.1582	Validation loss:	0.1171

Epoch 172/200	Training loss:	0.1764	Validation loss:	0.1163

Epoch 173/200	Training loss:	0.2041	Validation loss:	0.1172

Epoch 174/200	Training loss:	0.1613	Validation loss:	0.1187

Epoch 175/200	Training loss:	0.1699	Validation loss:	0.1171

Epoch 176/200	Training loss:	0.1612	Validation loss:	0.1177

Epoch 177/200	Training loss:	0.1428	Validation loss:	0.1184

Epoch 178/200	Training loss:	0.1617	Validation loss:	0.1185

Epoch 179/200	Training loss:	0.1651	Validation loss:	0.1165

Epoch 180/200	Training loss:	0.1501	Validation loss:	0.1180

Epoch 181/200	Training loss:	0.2049	Validation loss:	0.1170

Epoch 182/200	Training loss:	0.1359	Validation loss:	0.1168

Epoch 183/200	Training loss:	0.1631	Validation loss:	0.1180

Epoch 184/200	Training loss:	0.2263	Validation loss:	0.1168

Epoch 185/200	Training loss:	0.1894	Validation loss:	0.1181

Epoch 186/200	Training loss:	0.2530	Validation loss:	0.1168

Epoch 187/200	Training loss:	0.1530	Validation loss:	0.1164

Epoch 188/200	Training loss:	0.1318	Validation loss:	0.1180

Epoch 189/200	Training loss:	0.1562	Validation loss:	0.1177

Epoch 190/200	Training loss:	0.1489	Validation loss:	0.1167

Epoch 191/200	Training loss:	0.1687	Validation loss:	0.1164

Epoch 192/200	Training loss:	0.1898	Validation loss:	0.1173

Epoch 193/200	Training loss:	0.1457	Validation loss:	0.1167

Epoch 194/200	Training loss:	0.2520	Validation loss:	0.1168

Epoch 195/200	Training loss:	0.1834	Validation loss:	0.1176

Epoch 196/200	Training loss:	0.1987	Validation loss:	0.1156

Epoch 197/200	Training loss:	0.1680	Validation loss:	0.1171

Epoch 198/200	Training loss:	0.1548	Validation loss:	0.1178

Epoch 199/200	Training loss:	0.1647	Validation loss:	0.1169

Epoch 200/200	Training loss:	0.1600	Validation loss:	0.1170
